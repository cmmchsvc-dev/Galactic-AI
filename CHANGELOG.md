# Changelog â€” Galactic AI

All notable changes to Galactic AI are documented here.

---

## [v1.0.9] â€” 2026-02-22

### Added
- **ğŸ¬ Video Generation (Google Veo)** â€” Text-to-video and image-to-video via Google Veo API. Two new tools: `generate_video` (text prompt â†’ MP4) and `generate_video_from_image` (still image â†’ animated MP4). Supports 4s/6s/8s duration, 720p/1080p/4K resolution, 16:9 and 9:16 aspect ratios, negative prompts. Async polling with status updates during generation
- **ğŸ¥ Inline Video Player** â€” Generated videos appear inline in the Control Deck chat as HTML5 `<video>` elements with controls, autoplay, muted loop, and a download link. New `/api/video/{filename}` serving endpoint
- **ğŸ“¦ New NVIDIA Models** â€” Added Nemotron Super 49B (`nvidia/llama-3.3-nemotron-super-49b-v1.5`), Nemotron Nano 9B v2 (`nvidia/nvidia-nemotron-nano-9b-v2`), and Phi-3 Medium (`microsoft/phi-3-medium-4k-instruct`) to the Models page with model cards
- **ğŸ§  NVIDIA Thinking Model Params** â€” Added per-model reasoning parameters for DeepSeek V3.2 and Nemotron Nano 9B v2 to the `_NVIDIA_THINKING_MODELS` configuration
- **ğŸ¬ Multi-Provider Video Config** â€” New `video:` config section with Google Veo day-one support and scaffolding for Runway Gen-4, Kling, and Luma Dream Machine

### Fixed
- **ğŸ’¬ Chat/Logs/Thinking Scroll** â€” All three tabs now use conventional bottom-up chat ordering (newest content at bottom, like Facebook Messenger). Root cause: stream-bubble was the first child and `insertBefore(sb.nextSibling)` placed new messages at the top. Fixed by moving stream-bubble to be the last child and inserting before it. Also fixed log trim (now removes oldest, not newest) and filterLogs (removed reverse)
- **ğŸ”§ NVIDIA Streaming Hang** â€” Some NVIDIA models (Qwen 3.5 397B) accept streaming requests but never send SSE data. Added `_NVIDIA_NO_STREAM` set to force non-streaming, plus automatic streaming-to-non-streaming fallback for all NVIDIA models
- **â³ NVIDIA Cold-Start Retry** â€” Large NVIDIA models return HTTP 504 after ~5 minutes when cold-loading onto GPUs. Added auto-retry (up to 2 attempts) on 502/503/504 with 10s delay and "â³ NVIDIA model loading" status messages
- **ğŸ“¡ NVIDIA Granular Timeouts** â€” Changed from single 300s timeout to 30s connect + 600s read for large NVIDIA models that take several minutes to respond
- **ğŸ”— HuggingFace URL Migration** â€” Updated from deprecated `api-inference.huggingface.co/v1` to `router.huggingface.co/v1` across gateway, web deck, and config
- **ğŸ“„ Non-Streaming JSON Parsing** â€” Hardened non-streaming response path with HTTP status check before parsing and safe handling of empty response bodies
- **âš¡ Bulletproof Shutdown** â€” Added 8-second hard-exit timer that prevents infinite hang on shutdown; proper shutdown_event chain across all subsystems

---

## [v1.0.8] â€” 2026-02-22

### Fixed
- **ğŸ”§ Model Persistence â€” Definitive Fix** â€” Complete architectural overhaul of config save system. Root cause: `_save_config()` was a destructive full-file overwrite. Fix: safe read-modify-write pattern, defensive model-key writeback on every save, unified config paths via `self.core.config_path`, consolidated triple-save for toggle settings, startup diagnostics with `[config]`/`[DEFAULT]` source tags
- **ğŸ¨ Imagen 4 Safety Filter** â€” Fixed `400 INVALID_ARGUMENT` by changing `safety_filter_level` from `BLOCK_ONLY_HIGH` to `BLOCK_LOW_AND_ABOVE`
- **ğŸ–¼ï¸ Inline Image Display** â€” Added diagnostic logging to image delivery pipeline, fixed `_rawText` accumulation bug between messages
- **ğŸ’¾ Config Save Path Fixes** â€” `handle_save_key()`, `handle_setup()`, `handle_login()`, and `model_manager._save_config()` all now use safe read-modify-write pattern with defensive model writeback

---

## [v1.0.7] â€” 2026-02-21

### Added
- **ğŸ”½ Shutdown/Restart Buttons** â€” Control Deck now has shutdown and restart buttons for easy server management
- **ğŸ¨ Imagen 4 SDK Migration** â€” Migrated from legacy Gemini image API to the new `google-genai` SDK for Imagen 4 generation

### Fixed
- **ğŸ“œ Scroll Ordering** â€” Initial scroll ordering implementation (newest-first, later corrected in v1.0.9 to conventional bottom-up)
- **ğŸ¨ SD3.5 NVIDIA Fix** â€” Stable Diffusion 3.5 image generation restored on NVIDIA NIM
- **ğŸ¤– SubAgent Overhaul** â€” Reworked SubAgentManager for reliability and proper task tracking

---

## [v1.0.6] â€” 2026-02-21

### Fixed
- **ğŸ§  VAULT / Personality not loading** â€” `config.yaml` â†’ `paths.workspace` was still pointing to the old OpenClaw workspace directory after install migration. The personality system (`personality.py`) reads VAULT.md, IDENTITY.md, USER.md, SOUL.md, and MEMORY.md from the workspace path. With the stale path, the AI had no access to the user's vault (credentials, personal data) and responded with "I don't have access to your personal credentials." Fixed by updating the workspace path to the current install directory
- **ğŸ¯ Smart routing misclassification of file uploads** â€” When a user sends a document via Telegram (e.g., CHANGELOG.md, README.md), the entire file content was fed into `classify_task()` for smart routing. Any .md file describing code changes would contain keywords like "script", "function", "implement", triggering a "coding" classification and routing to the Qwen Coder 480B model â€” even when the user wanted help with marketing or social media. `classify_task()` now strips attached file content and code blocks before classification, so routing is based on the user's actual message/caption, not file contents
- **â± Telegram timeout killing active tasks early** â€” Telegram bridge had its own `timeout_seconds: 180` that wrapped `speak()` in a separate `asyncio.wait_for()`. The global `speak_timeout` is 600s, but Telegram's 180s limit killed the task before the gateway finished. `_get_speak_timeout()` now uses `max(global_timeout, telegram_timeout)` so the Telegram bridge never cuts off a task that the gateway is still allowed to work on

---

## [v1.0.5] â€” 2026-02-21

### Added
- **ğŸ”Œ Agent Loop Circuit Breaker** â€” After 3 consecutive tool failures (errors or timeouts), the AI is forced to stop calling tools and explain the situation to the user instead of spiraling through all 50 turns
- **âš ï¸ Progressive Backpressure** â€” At 50% and 80% of the tool-turn budget, the AI receives nudge messages telling it to wrap up and deliver results, preventing runaway automation sessions
- **ğŸ”„ Tool Repetition Guard** â€” If the same tool is called 4+ times in a 6-call window without progress, the AI is instructed to change strategy or explain the problem
- **ğŸ”’ Model Lock During Active Tasks** â€” Switching models via the Control Deck while the AI is mid-task now queues the switch instead of disrupting the active conversation (applied automatically after the task completes)
- **ğŸ¯ Smart Routing Restoration** â€” When smart routing temporarily switches to a specialized model (e.g., Qwen Coder for coding tasks), the original model is now automatically restored after the request completes

### Fixed
- **Agent timeout spiral** â€” Complex tasks (like script creation) could burn through all 50 tool turns without converging, hitting the 600s wall-clock timeout. The new anti-spin guardrails (circuit breaker, backpressure, repetition guard) prevent this pattern
- **Smart routing model leak** â€” `auto_route()` switched the model but never restored it, so the specialized model stuck around for subsequent unrelated requests

---

## [v1.0.4] â€” 2026-02-21

### Fixed
- **ğŸ”§ Model persistence across restarts** â€” Selected primary model now survives restarts. Two bugs were causing the model to revert to Gemini 2.5 Flash on every startup:
  1. `/api/switch_model` (used by the Models tab quick-switch) updated the live session only â€” it never wrote the selection to `config.yaml`. It now calls `ModelManager._save_config()` so the choice is immediately persisted.
  2. `GalacticGateway.__init__` read `config.gateway.model` which was only written by the Settings tab path, not the Models tab path. It now reads `config.models.primary_model` first (the canonical value written by `ModelManager`), falling back to `config.gateway.model`, so startup always loads the correct last-used model regardless of which UI element made the switch.

---

## [v1.0.3] â€” 2026-02-21

### Added
- **ğŸ¤ Voice Input Button** â€” Microphone button in the Control Deck chat bar. Click to record, sends audio to Whisper (OpenAI/Groq) for transcription, inserts text into the chat input automatically
- **ğŸ”¥ Auto Windows Firewall Rule** â€” On startup with `remote_access: true`, Galactic AI automatically adds a Windows Firewall inbound rule allowing TCP traffic on the Control Deck port (private networks only)
- **"CONTROL DECK" label** in the top bar next to the model status badge

### Fixed
- **Remote access HTTP mode** â€” Server now binds to `0.0.0.0` on plain HTTP instead of HTTPS with self-signed TLS. Self-signed certs caused `ERR_EMPTY_RESPONSE`. JWT authentication still protects all remote API endpoints
- **Updater em dash encoding** â€” Fixed `update.ps1` parse error caused by em dash character corruption in some environments

---

## [v1.0.2] â€” 2026-02-21

### Added
- **Localhost bypass for remote auth** â€” Local connections from `127.0.0.1`/`::1` bypass JWT auth so the PC is never locked out of the Control Deck when `remote_access: true`
- **"CONTROL DECK" label** in top bar (first introduced here, improved in v1.0.3)

### Fixed
- **QR code compatibility** â€” QR pairing code now uses standard black-on-white colors with higher error correction (`ERROR_CORRECT_H`)
- **Test Voice button now plays audio** â€” Previously only generated the MP3 server-side without streaming it back. Now uses `/api/tts` to stream audio bytes to the browser and plays them directly
- **Desktop shortcut icon** â€” `galactic_ai_flux_v4.ico` added to the repository (was missing, referenced by `create_shortcut.ps1`)

---

## [v1.0.1] â€” 2026-02-21

### Added
- **Config auto-migration** â€” On startup, `load_config()` detects missing config sections from newer versions and adds them with safe defaults. Affected sections: `gmail`, `discord`, `whatsapp`, `webhooks`, `web`, `elevenlabs`, `models`, `tool_timeouts`, `aliases`. Existing values are never overwritten
- **Updater `-Force` flag** â€” `.\update.ps1 -Force` and `./update.sh --force` re-download even when the installed version matches the latest release

### Fixed
- Missing release ZIP assets â€” Added `windows.zip`, `macos.zip`, `linux.tar.gz`, `universal.zip`, and `SHA256SUMS.txt`

---

## [v1.0.0] â€” 2026-02-21

### Added
- **ğŸŒ Remote Access Mode** â€” Access Galactic AI from anywhere
  - Enable with `remote_access: true` in config.yaml
  - Auto-generated self-signed TLS certificates (HTTPS)
  - Binds to `0.0.0.0` for LAN/internet access
  - Startup warning when remote access is active
- **ğŸ”‘ JWT Authentication** â€” Enterprise-grade auth for remote connections
  - HMAC-SHA256 signed tokens with 24-hour expiry
  - Auto-generated 64-character hex secret stored in config.yaml
  - Auth middleware on all `/api/*` endpoints
  - WebSocket authentication via query parameter
  - Backward-compatible with existing password hash for local mode
- **ğŸ›¡ï¸ Rate Limiting** â€” Brute-force protection
  - 60 requests/minute per IP for API endpoints
  - 5 login attempts/minute per IP
  - Returns 429 with `Retry-After` header
- **ğŸ”’ CORS Middleware** â€” Cross-origin protection with configurable allowed origins
- **ğŸ™ï¸ Voice API Endpoints**:
  - `POST /api/tts` â€” text-to-speech via existing ElevenLabs/edge-tts/gTTS pipeline, returns MP3
  - `POST /api/stt` â€” speech-to-text via OpenAI Whisper with Groq Whisper fallback, accepts multipart audio
- **`remote_access.py`** â€” New security module centralizing JWT, rate limiting, CORS, and auth middleware

### Fixed
- **Settings model save bug** â€” Changing primary/fallback models in the Settings tab now takes effect immediately
  - `switch_to_primary()` no longer short-circuits when already in primary mode
  - `_save_config()` now syncs gateway provider/model in config.yaml for persistence across restarts

### Changed
- Version bumped from v0.9.3 to v1.0.0 across all files
- `web_deck.py` login endpoint returns JWT tokens when remote access is enabled
- `web_deck.py` JavaScript uses `authFetch()` wrapper for JWT auth headers on all API calls
- `web_deck.py` WebSocket uses `wss://` protocol when on HTTPS
- `galactic_core_v2.py` auto-generates JWT secret on first remote-mode startup
- Website `index.html` updated with remote access section

---

## [v0.9.3] â€” 2026-02-21

### Added
- **âš™ï¸ Settings Tab** â€” New Control Deck tab with three sections:
  - *Model Configuration* â€” Primary and fallback provider+model dropdowns (populated from all 100+ models), auto-fallback toggle, smart routing toggle, streaming toggle
  - *Voice* â€” TTS voice dropdown with all 7 voices + Test Voice button
  - *System* â€” GitHub update check interval, speak() timeout, max ReAct turns
  - All settings saved immediately to `config.yaml` via new API endpoints
- **ğŸ” VAULT.md** â€” Private credentials file for automation tasks
  - `VAULT-example.md` template included in repository
  - Loaded by `personality.py` into every system prompt with "never share or expose" instruction
  - Gitignored and protected by both `update.ps1` and `update.sh`
  - Editable in the Memory tab of the Control Deck
- **ğŸ—£ï¸ TTS Voice Selector** â€” Quick Tools sidebar dropdown for instant voice switching (Guy, Aria, Jenny, Davis, Nova, Byte, gTTS)
- **ğŸ†• GitHub Auto-Update Checker** â€” Background task checks `cmmchsvc-dev/Galactic-AI` releases every 6 hours (configurable, 0 = disabled). Shows dismissible banner + 30-second toast in Control Deck when update available
- **ğŸ”½ Model Dropdowns** â€” PER-MODEL OVERRIDES now uses `<select>` dropdown populated from ALL_MODELS instead of a text input. Custom model text input provided as fallback
- **3 new API endpoints**: `POST /api/settings/models`, `POST /api/settings/voice`, `POST /api/settings/system`
- **`voice` and `update_check_interval`** fields added to `/api/status` response
- **VAULT.md** added to workspace file lists in Memory tab (OpenClaw migration, file list, auto-create defaults)
- **`system.update_check_interval: 21600`** added to `config.yaml`

### Changed
- Settings tab allows switching primary/fallback models without leaving the browser â€” no more editing `config.yaml` manually
- `personality.py` `get_system_prompt()` now loads VAULT.md as the 5th injected file
- `galactic_core_v2.py` `imprint_workspace()` now includes VAULT.md in the workspace files list
- `update.ps1` and `update.sh` protected file lists updated to include VAULT.md
- `.gitignore` updated to explicitly list VAULT.md
- Website `index.html` updated to v0.9.3 with new features section
- `docs/ARCHITECTURE.md` fully rewritten to reflect v0.9.3 system design
- Tool count updated to 100+ across README, FEATURES, and website

---

## [v0.9.2] â€” 2026-02-20

### Added
- **Resilient model fallback chain** â€” Error-type-specific cooldowns (RATE_LIMIT: 60s, SERVER_ERROR: 30s, TIMEOUT: 10s, AUTH_ERROR: 86400s, QUOTA_EXHAUSTED: 3600s)
- **Automatic provider recovery** â€” Background loop retests failed providers after cooldown expires
- **16 new built-in tools** (108 total):
  - Archives: `zip_create`, `zip_extract`
  - HTTP: `http_request` (raw REST with custom headers)
  - Environment: `env_get`, `env_set`
  - Window management: `window_list`, `window_focus`, `window_resize`
  - System: `system_info`, `kill_process_by_name`
  - Utilities: `qr_generate`, `color_pick`, `text_transform` (15 text operations)
  - Notifications: `notify` (desktop toast/balloon)
  - Clipboard: `clipboard_get`, `clipboard_set`
- **Expanded Status screen** â€” 30+ telemetry fields across 6 sections (Model, Fallback Chain, Runtime, Memory, Tokens, Plugins)
- **speak() wall-clock timeout** â€” Entire ReAct loop wrapped in `asyncio.wait_for()`, default 600s, configurable via `models.speak_timeout`
- **Per-tool configurable timeouts** in `config.yaml` under `tool_timeouts` (exec_shell: 120s, execute_python: 60s, generate_image: 180s)
- **Shell command timeout** in ShellExecutor plugin
- **`model_fallback` WebSocket event** â€” Control Deck shows toast notification when provider falls back
- **Toast notification system** â€” CSS-animated popups for model fallback events

### Changed
- `config.yaml` expanded with `tool_timeouts`, `speak_timeout`, `fallback_cooldowns` sections
- Status tab HTML redesigned with 6 organized sections

---

## [v0.9.1] â€” 2026-02-14

### Added
- **Organized image folders** â€” Generated images saved to date-stamped subdirectories
- **Structured logging system** â€” Daily JSON component logs alongside plain-text system_log.txt
- **Log rotation** â€” Files trimmed at 2MB / 5000 lines

### Changed
- Log system backwards-compatible â€” existing callers unchanged

---

## [v0.9.0] â€” 2026-02-10

### Added
- **Discord bridge** â€” Full bot integration with slash commands, typing indicators, allowed-channel access control
- **WhatsApp bridge** â€” Meta Cloud API webhook integration
- **Gmail bridge** â€” IMAP inbox monitoring with Telegram notifications
- **Imagen 4 / Imagen 4 Ultra** â€” Google Imagen 4 image generation tools (`generate_image_gemini`, `generate_image_gemini_ultra`)
- **Imagen 4 Fast** â€” Fast variant via Gemini API
- **Telegram image model selector** â€” `/model` â†’ Image Models in Telegram to switch between Imagen 4 Ultra, Imagen 4, FLUX.1 Dev, Imagen 4 Fast, FLUX.1 Schnell
- **Thinking tab persistence** â€” Agent trace buffered in memory (last 500 entries), restored on page load via `/api/traces`
- **Chat timestamps** â€” HH:MM:SS timestamp on every message
- **All providers in Telegram model menu** â€” 14 providers Ã— their model lists in `/model` keyboard
- **Image attachment in chat** â€” Attach images to chat messages for vision analysis

### Fixed
- Graceful shutdown â€” single Ctrl+C now cleanly closes all subsystems
- Per-tool timeout â€” 60s `asyncio.wait_for` on every tool call prevents "typing forever"

---

## [v0.8.1] â€” 2026-01-28

### Fixed
- Typing indicator heartbeat â€” no longer sends duplicate "typing" events
- Fast Ctrl+C shutdown â€” no longer hangs waiting for Telegram long-poll to expire
- Duplicate message guard â€” prevents double-processing of messages on slow connections

---

## [v0.8.0] â€” 2026-01-20

### Added
- 17 new tools â€” clipboard, notifications, window management, HTTP requests, QR codes, system info, text transforms, SD3.5 image gen, FLUX auto-generate
- FLUX.1 Schnell and FLUX.1 Dev image generation via NVIDIA NIM
- Stable Diffusion 3.5 Large image generation
- FLUX auto-generate mode â€” typing any prompt generates an image when FLUX is selected

---

## [v0.7.9] â€” 2026-01-12

### Added
- Image delivery to Telegram and Control Deck â€” generated images sent as photos, not file paths
- Dual FLUX API keys â€” separate keys for FLUX.1 Schnell and FLUX.1 Dev

---

## [v0.7.8] â€” 2026-01-08

### Added
- 9 new NVIDIA models (Kimi K2.5, GLM5, MiniMax M2, Nemotron variants)
- Thinking model support (models that return `<thinking>` blocks)
- File attachment fix in chat

---

## [v0.7.7] â€” 2025-12-28

### Added
- Accessibility-driven browser interactions â€” click/type by accessibility ref ID
- Network request interception and response body capture

---

## [v0.7.6] â€” 2025-12-20

### Added
- Desktop automation plugin (pyautogui) â€” click, type, scroll, drag, template matching
- Clipboard tools

---

## [v0.7.5] â€” 2025-12-14

### Added
- Sub-agent orchestration â€” spawn parallel AI agents for multi-step workflows
- `SubAgentManager` plugin

---

## [v0.7.4] â€” 2025-12-08

### Added
- Browser session save/restore â€” persist cookies and storage state across runs
- Geolocation spoofing, proxy support, media emulation

---

## [v0.7.3] â€” 2025-12-02

### Added
- Browser tracing (Playwright trace recording)
- Iframe support â€” execute actions inside nested frames
- Browser storage tools (localStorage, sessionStorage)

---

## [v0.7.2] â€” 2025-11-25

### Added
- NVIDIA single-key setup â€” one key works for all NVIDIA-hosted models
- Quick-pick model chips in Control Deck
- Custom model text field for Ollama custom models
- Ollama 10-minute timeout for large local models

---

## [v0.7.1] â€” 2025-11-18

### Added
- **Persistent memory** â€” MEMORY.md + memory_aura.json
- **Voice I/O** â€” Whisper transcription + TTS response via Telegram
- **Chat persistence** â€” `logs/chat_history.jsonl`, restored on page load
- **Personality config** â€” byte / custom / generic / files modes
- **One-command auto-updater** â€” `update.ps1` and `update.sh`

---

## [v0.7.0] â€” 2025-11-10

### Added
- 14 AI providers (added Cerebras, OpenRouter, HuggingFace, Together AI, Perplexity)
- TTS configuration in Setup Wizard
- OpenClaw migration step â€” import existing memory/identity files

### Fixed
- Gemini duplicate response bug

---

## [v0.6.0-Alpha] â€” 2025-10-28

### Initial public release
- 72 built-in tools
- 5 AI providers (Google, Anthropic, OpenAI, Groq, Ollama)
- Telegram bot
- Web Control Deck at localhost:17789
- ReAct agentic loop
- Playwright browser automation
